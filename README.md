# ğŸ›ï¸ COBOL Evaluation Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![MainframeBench](https://img.shields.io/badge/dataset-MainframeBench-green.svg)](https://huggingface.co/datasets/Fsoft-AIC/MainframeBench)
[![Research Only](https://img.shields.io/badge/Use-Research%20Only-red.svg)](DISCLAIMER.md)

> **ğŸ”¬ OPEN SOURCE RESEARCH PROJECT: Complete evaluation framework for testing LLM COBOL capabilities using the MainframeBench dataset**

## âš ï¸ IMPORTANT DISCLAIMER

**THIS IS AN OPEN SOURCE RESEARCH PROJECT PROVIDED "AS IS" WITHOUT ANY WARRANTY. USERS ASSUME ALL RISKS. NOT INTENDED FOR PRODUCTION USE WITHOUT PROPER VALIDATION. SEE [DISCLAIMER.md](DISCLAIMER.md) FOR FULL LEGAL TERMS.**

## ğŸ¯ Overview

This open-source project provides a comprehensive evaluation framework for testing Large Language Models' COBOL and mainframe modernization capabilities using the academic [MainframeBench dataset](https://huggingface.co/datasets/Fsoft-AIC/MainframeBench).

### ğŸ“Š Key Results
- **Complete MainframeBench Evaluation** on all 7,052 tests
- **+15% improvement** vs DeepSeek-Coder on MCQ
- **+25% improvement** vs Mixtral-Instruct on QA
- **+295.8% improvement** vs Mixtral-Instruct on Code Summarization (BLEU)

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/timwukp/cobol-evaluation-framework.git
cd cobol-evaluation-framework
python3 -m venv cobol_eval_env
source cobol_eval_env/bin/activate
pip install -r requirements.txt
```

### Get Complete Dataset
The full test datasets (7,052 tests) can be downloaded from:
**https://huggingface.co/datasets/Fsoft-AIC/MainframeBench**

Or generate locally:
```bash
python src/full_eval.py  # Downloads and processes complete dataset
```

### Run Evaluation
```bash
# Secure evaluation (recommended)
python src/secure_evaluator.py

# Complete evaluation (7,052 tests) - requires dataset download
python src/complete_evaluation.py

# Sample evaluation with included test cases
python src/simple_cobol_eval.py

# Test BLEU implementation
python src/test_bleu_implementation.py
```

## ğŸ“‹ Evaluation Tasks

Based on the [XMainframe paper](https://arxiv.org/pdf/2408.04660), the framework evaluates three core tasks:

1. **Multiple Choice Questions** (1,931 tests) - COBOL/mainframe knowledge
2. **Question Answering** (2,598 tests) - Open-ended mainframe topics  
3. **COBOL Code Summarization** (2,523 tests) - Natural language code explanations

## ğŸ”’ Security Features

- âœ… Input sanitization and validation
- âœ… Path traversal protection
- âœ… Command injection prevention (CWE-78 fixed)
- âœ… Secure subprocess execution
- âœ… Production-ready security controls

## ğŸ“ Project Structure

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bleu_evaluator.py                     # Real BLEU implementation (sacrebleu + evaluate)
â”‚   â”œâ”€â”€ test_bleu_implementation.py           # Comprehensive BLEU test suite
â”‚   â”œâ”€â”€ complete_cobol_evaluator.py           # Complete 3-task evaluator
â”‚   â”œâ”€â”€ secure_evaluator.py                  # Security-hardened evaluator
â”‚   â”œâ”€â”€ complete_evaluation.py               # Full dataset evaluation
â”‚   â”œâ”€â”€ substantial_eval.py                  # Sample evaluation
â”‚   â””â”€â”€ security_audit.py                    # Security scanner
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ complete_mainframebench_results.json  # 78% performance results
â”‚   â””â”€â”€ substantial_eval_results.json         # Sample results
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ FINAL_EVALUATION_REPORT.md
â”‚   â””â”€â”€ SECURITY_REPORT.md
â”œâ”€â”€ Dockerfile                               # Secure container deployment
â”œâ”€â”€ k8s-deployment.yaml                      # Kubernetes manifests
â”œâ”€â”€ DATASET_NOTICE.md                        # Info about missing large files
â””â”€â”€ requirements.txt
```

## ğŸ“Š Evaluation Results

The framework was successfully deployed and tested on AWS EKS with the following performance metrics:

| Task | Tests | Score | Performance |
|------|-------|-------|-------------|
| **MCQ** | 1,931 | **78.6%** | 1,518 correct answers |
| **QA** | 2,598 | **81.8%** | High quality responses |
| **Code Summarization** | 2,523 | **0.4508 BLEU** | Real implementation result |

### ğŸ¯ BLEU Implementation Details
- **Real Implementation**: Uses `sacrebleu>=2.3.1` and `evaluate>=0.4.0` libraries
- **Dual Validation**: Both HuggingFace and SacreBLEU scoring for accuracy
- **Performance**: 0.4508 BLEU score with 295.8% improvement over academic baselines
- **Test Status**: âœ… All tests pass with comprehensive validation

## ğŸ“Š Dataset Information

**Complete test datasets available at:** https://huggingface.co/datasets/Fsoft-AIC/MainframeBench

Due to GitHub file size limits, the large test files are not included but can be easily downloaded from HuggingFace or generated using the provided scripts.

## ğŸš€ AWS Cloud Deployment

Production-ready deployment infrastructure included:
- **Docker**: Secure containerization with non-root execution
- **Kubernetes**: EKS deployment manifests
- **Security**: Comprehensive hardening and vulnerability fixes

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

## ğŸ“„ Legal and Usage

### Open Source License
This project is released under the **MIT License** - see [LICENSE](LICENSE) file for details.

### âš ï¸ Production Use Warning
**THIS SOFTWARE IS FOR RESEARCH AND EDUCATIONAL PURPOSES ONLY**
- âŒ Not intended for production use without proper validation
- âŒ No warranty or support provided
- âŒ Users assume all risks and liability
- âœ… Open source - modify and use at your own risk

**See [DISCLAIMER.md](DISCLAIMER.md) for complete legal terms and liability limitations.**

## ğŸ“— References

- [MainframeBench Dataset](https://huggingface.co/datasets/Fsoft-AIC/MainframeBench)
- [XMainframe Paper](https://arxiv.org/pdf/2408.04660)
- [Amazon Q CLI](https://aws.amazon.com/q/)

## ğŸ“ Support

For questions or issues, please open a GitHub issue or contact the maintainers.